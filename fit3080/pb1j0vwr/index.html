<!doctype html><html lang="en-US"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="generator" content="VuePress 2.0.0-rc.24" /><meta name="theme" content="VuePress Theme Plume 1.0.0-rc.158" /><script id="check-mac-os">document.documentElement.classList.toggle('mac', /Mac|iPhone|iPod|iPad/i.test(navigator.platform))</script><script id="check-dark-mode">;(function () {const um= localStorage.getItem('vuepress-theme-appearance') || 'auto';const sm = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;const isDark = um === 'dark' || (um !== 'light' && sm);document.documentElement.dataset.theme = isDark ? 'dark' : 'light';})();</script><link rel="icon" href="/wan-s-blog/favicon.ico"><link rel="icon" href="/favicon.ico"><link rel="apple-touch-icon" sizes="180x180" href="/wan-s-blog/favicon.ico"><link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico"><title>Intelligent Agents and Rationality | WAN's Blog</title><meta name="description" content="WAN's blog"><link rel="preload" href="/wan-s-blog/assets/style-Dlml69BT.css" as="style"><link rel="stylesheet" href="/wan-s-blog/assets/style-Dlml69BT.css"><link rel="modulepreload" href="/wan-s-blog/assets/app-C7AwxLk6.js"><link rel="modulepreload" href="/wan-s-blog/assets/index.html-B8Hd2t-d.js"></head><body><div id="app"><!--[--><!--[--><div class="theme-plume vp-layout" vp-container data-v-d90a7a26><!--[--><!--[--><!--]--><!--[--><span tabindex="-1" data-v-d5a8d0bc></span><a href="#VPContent" class="vp-skip-link visually-hidden" data-v-d5a8d0bc> Skip to content </a><!--]--><!----><header class="vp-nav" data-v-d90a7a26 data-v-e98a6132><div class="vp-navbar" vp-navbar data-v-e98a6132 data-v-2c31ea5e><div class="wrapper" data-v-2c31ea5e><div class="container" data-v-2c31ea5e><div class="title" data-v-2c31ea5e><div class="vp-navbar-title has-sidebar" data-v-2c31ea5e data-v-1a4f50af><a class="vp-link link no-icon title" href="/wan-s-blog/" data-v-1a4f50af><!--[--><!--[--><!--]--><!----><span data-v-1a4f50af>WAN&#39;s Blog</span><!--[--><!--]--><!--]--><!----></a></div></div><div class="content" data-v-2c31ea5e><div class="content-body" data-v-2c31ea5e><!--[--><!--]--><div class="vp-navbar-search search" data-v-2c31ea5e><div class="search-wrapper" data-v-97535d1e><!----><div id="local-search" data-v-97535d1e><button type="button" class="mini-search mini-search-button" aria-label="Search" data-v-97535d1e><span class="mini-search-button-container"><span class="mini-search-search-icon vpi-mini-search" aria-label="search icon"></span><span class="mini-search-button-placeholder">Search</span></span><span class="mini-search-button-keys"><kbd class="mini-search-button-key"></kbd><kbd class="mini-search-button-key">K</kbd></span></button></div></div></div><!--[--><!--]--><nav aria-labelledby="main-nav-aria-label" class="vp-navbar-menu menu" data-v-2c31ea5e data-v-d43c1732><span id="main-nav-aria-label" class="visually-hidden" data-v-d43c1732>Main Navigation</span><!--[--><!--[--><a class="vp-link link navbar-menu-link" href="/wan-s-blog/" tabindex="0" data-v-d43c1732 data-v-d4acf911><!--[--><span class="vp-icon vpi-2gpyrm9q" style="" aria-hidden data-provider="iconify" data-v-d4acf911></span><span data-v-d4acf911>Home</span><!----><!--]--><!----></a><!--]--><!--[--><a class="vp-link link navbar-menu-link" href="/wan-s-blog/blog/" tabindex="0" data-v-d43c1732 data-v-d4acf911><!--[--><span class="vp-icon vpi-87rag0z2" style="" aria-hidden data-provider="iconify" data-v-d4acf911></span><span data-v-d4acf911>Blog</span><!----><!--]--><!----></a><!--]--><!--[--><a class="vp-link link navbar-menu-link" href="/wan-s-blog/blog/tags/" tabindex="0" data-v-d43c1732 data-v-d4acf911><!--[--><span class="vp-icon vpi-s5w0hi5n" style="" aria-hidden data-provider="iconify" data-v-d4acf911></span><span data-v-d4acf911>Tags</span><!----><!--]--><!----></a><!--]--><!--[--><a class="vp-link link navbar-menu-link" href="/wan-s-blog/blog/archives/" tabindex="0" data-v-d43c1732 data-v-d4acf911><!--[--><span class="vp-icon vpi-yf0gznx9" style="" aria-hidden data-provider="iconify" data-v-d4acf911></span><span data-v-d4acf911>Archives</span><!----><!--]--><!----></a><!--]--><!--[--><a class="vp-link link navbar-menu-link" href="/wan-s-blog/notes/" tabindex="0" data-v-d43c1732 data-v-d4acf911><!--[--><span class="vp-icon vpi-n0z1b697" style="" aria-hidden data-provider="iconify" data-v-d4acf911></span><span data-v-d4acf911>Notes</span><!----><!--]--><!----></a><!--]--><!--]--></nav><!--[--><!--]--><div class="vp-flyout vp-navbar-translations translations" data-v-2c31ea5e data-v-0d45479b data-v-86530b6c><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="Languages" data-v-86530b6c><span class="text" data-v-86530b6c><!----><span class="vpi-languages option-icon" data-v-86530b6c></span><!----><!----><span class="vpi-chevron-down text-icon" data-v-86530b6c></span></span></button><div class="menu" data-v-86530b6c><div class="vp-menu" data-v-86530b6c data-v-709dc2b1><!----><!--[--><!--[--><div class="items" data-v-0d45479b><p class="title" data-v-0d45479b>English</p><!--[--><div class="vp-menu-link" data-v-0d45479b data-v-1ff1855f><a class="vp-link link" href="/wan-s-blog/zh-TW/" data-v-1ff1855f><!--[--><!----> 繁體中文 <!----><!--]--><!----></a></div><div class="vp-menu-link" data-v-0d45479b data-v-1ff1855f><a class="vp-link link" href="/wan-s-blog/zh-CN/" data-v-1ff1855f><!--[--><!----> 简体中文 <!----><!--]--><!----></a></div><!--]--></div><!--]--><!--]--></div></div></div><div class="vp-navbar-appearance appearance" data-v-2c31ea5e data-v-a295abf6><button class="vp-switch vp-switch-appearance" type="button" role="switch" title aria-checked="false" data-v-a295abf6 data-v-596c25a9 data-v-7eb32327><span class="check" data-v-7eb32327><span class="icon" data-v-7eb32327><!--[--><span class="vpi-sun sun" data-v-596c25a9></span><span class="vpi-moon moon" data-v-596c25a9></span><!--]--></span></span></button></div><div class="vp-social-links vp-navbar-social-links social-links" data-v-2c31ea5e data-v-ad52545c data-v-40bac536><!--[--><a class="vp-social-link no-icon" href="https://github.com/yunfenglong" aria-label="github" target="_blank" rel="noopener" data-v-40bac536 data-v-67b21932><span class="vpi-social-github" /></a><!--]--></div><div class="vp-flyout vp-navbar-extra extra" data-v-2c31ea5e data-v-652282fd data-v-86530b6c><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-86530b6c><span class="vpi-more-horizontal icon" data-v-86530b6c></span></button><div class="menu" data-v-86530b6c><div class="vp-menu" data-v-86530b6c data-v-709dc2b1><!----><!--[--><!--[--><!----><div class="group" data-v-652282fd><div class="item appearance" data-v-652282fd><p class="label" data-v-652282fd>Appearance</p><div class="appearance-action" data-v-652282fd><button class="vp-switch vp-switch-appearance" type="button" role="switch" title aria-checked="false" data-v-652282fd data-v-596c25a9 data-v-7eb32327><span class="check" data-v-7eb32327><span class="icon" data-v-7eb32327><!--[--><span class="vpi-sun sun" data-v-596c25a9></span><span class="vpi-moon moon" data-v-596c25a9></span><!--]--></span></span></button></div></div></div><div class="group" data-v-652282fd><div class="item social-links" data-v-652282fd><div class="vp-social-links social-links-list" data-v-652282fd data-v-40bac536><!--[--><a class="vp-social-link no-icon" href="https://github.com/yunfenglong" aria-label="github" target="_blank" rel="noopener" data-v-40bac536 data-v-67b21932><span class="vpi-social-github" /></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="vp-navbar-hamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="nav-screen" data-v-2c31ea5e data-v-2b50024d><span class="container" data-v-2b50024d><span class="top" data-v-2b50024d></span><span class="middle" data-v-2b50024d></span><span class="bottom" data-v-2b50024d></span></span></button></div></div></div></div><div class="divider" data-v-2c31ea5e><div class="divider-line" data-v-2c31ea5e></div></div></div><!----></header><div class="vp-local-nav reached-top" data-v-d90a7a26 data-v-3944d8e8><button class="menu" aria-expanded="false" aria-controls="SidebarNav" data-v-3944d8e8><span class="vpi-align-left menu-icon" data-v-3944d8e8></span><span class="menu-text" data-v-3944d8e8>Menu</span></button><div class="vp-local-nav-outline-dropdown" style="--vp-vh:0px;" data-v-3944d8e8 data-v-4114a62c><button data-v-4114a62c>Return to top</button><!----></div></div><aside class="vp-sidebar" vp-sidebar data-v-d90a7a26 data-v-95211354><div class="curtain" data-v-95211354></div><nav id="SidebarNav" class="nav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-95211354><span id="sidebar-aria-label" class="visually-hidden" data-v-95211354> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-473fd05b><section class="vp-sidebar-item sidebar-item level-0" data-v-473fd05b data-v-12048f0f><!----><div data-v-12048f0f data-v-12048f0f><div class="items" data-v-12048f0f><!--[--><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-12048f0f data-v-12048f0f><div class="item" data-v-12048f0f><div class="indicator" data-v-12048f0f></div><!----><a class="vp-link link link" href="/wan-s-blog/fit3080/" data-v-12048f0f><!--[--><p class="text" data-v-12048f0f><span data-v-12048f0f>Overview</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-12048f0f data-v-12048f0f><div class="item" data-v-12048f0f><div class="indicator" data-v-12048f0f></div><!----><a class="vp-link link link" href="/wan-s-blog/fit3080/94l7fkfk/" data-v-12048f0f><!--[--><p class="text" data-v-12048f0f><span data-v-12048f0f>Intro to AI</span><!----></p><!--]--><!----></a><!----></div><!----></div><!--]--></div></div></section></div><div class="no-transition group" data-v-473fd05b><section class="vp-sidebar-item sidebar-item level-0 has-active" data-v-473fd05b data-v-12048f0f><div class="item" role="button" tabindex="0" data-v-12048f0f><div class="indicator" data-v-12048f0f></div><!----><h2 class="text" data-v-12048f0f><span data-v-12048f0f>Agents</span><!----></h2><!----></div><div data-v-12048f0f data-v-12048f0f><div class="items" data-v-12048f0f><!--[--><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-12048f0f data-v-12048f0f><div class="item" data-v-12048f0f><div class="indicator" data-v-12048f0f></div><!----><a class="vp-link link link" href="/wan-s-blog/fit3080/pb1j0vwr/" data-v-12048f0f><!--[--><p class="text" data-v-12048f0f><span data-v-12048f0f>Intelligent Agents and Rationality</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-12048f0f data-v-12048f0f><div class="item" data-v-12048f0f><div class="indicator" data-v-12048f0f></div><!----><a class="vp-link link link" href="/wan-s-blog/fit3080/1tgvq8kr/" data-v-12048f0f><!--[--><p class="text" data-v-12048f0f><span data-v-12048f0f>Real World Implementation</span><!----></p><!--]--><!----></a><!----></div><!----></div><!--]--></div></div></section></div><div class="no-transition group" data-v-473fd05b><section class="vp-sidebar-item sidebar-item level-0" data-v-473fd05b data-v-12048f0f><div class="item" role="button" tabindex="0" data-v-12048f0f><div class="indicator" data-v-12048f0f></div><!----><h2 class="text" data-v-12048f0f><span data-v-12048f0f>Search</span><!----></h2><!----></div><div data-v-12048f0f data-v-12048f0f><div class="items" data-v-12048f0f><!--[--><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-12048f0f data-v-12048f0f><div class="item" data-v-12048f0f><div class="indicator" data-v-12048f0f></div><!----><a class="vp-link link link" href="/wan-s-blog/fit3080/o9dc7deh/" data-v-12048f0f><!--[--><p class="text" data-v-12048f0f><span data-v-12048f0f>Solving Problems by Searching</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-12048f0f data-v-12048f0f><div class="item" data-v-12048f0f><div class="indicator" data-v-12048f0f></div><!----><a class="vp-link link link" href="/wan-s-blog/fit3080/ywm6p15r/" data-v-12048f0f><!--[--><p class="text" data-v-12048f0f><span data-v-12048f0f>Search Algorithm</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-12048f0f data-v-12048f0f><div class="item" data-v-12048f0f><div class="indicator" data-v-12048f0f></div><!----><a class="vp-link link link" href="/wan-s-blog/fit3080/60yndbd3/" data-v-12048f0f><!--[--><p class="text" data-v-12048f0f><span data-v-12048f0f>Romania (Application)</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-12048f0f data-v-12048f0f><div class="item" data-v-12048f0f><div class="indicator" data-v-12048f0f></div><!----><a class="vp-link link link" href="/wan-s-blog/fit3080/mugok8ho/" data-v-12048f0f><!--[--><p class="text" data-v-12048f0f><span data-v-12048f0f>Iterative Deepening Search</span><!----></p><!--]--><!----></a><!----></div><!----></div><!--]--></div></div></section></div><!--]--><!--[--><!--]--></nav></aside><!--[--><div id="VPContent" vp-content class="vp-content has-sidebar" data-v-d90a7a26 data-v-b2beaca7><div class="vp-doc-container has-sidebar has-aside" data-v-b2beaca7 data-v-23f6ad98><!--[--><!--]--><div class="container" data-v-23f6ad98><div class="aside" vp-outline data-v-23f6ad98><div class="aside-curtain" data-v-23f6ad98></div><div class="aside-container" data-v-23f6ad98><div class="aside-content" data-v-23f6ad98><div class="vp-doc-aside" data-v-23f6ad98 data-v-5976474c><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="vp-doc-aside-outline" role="navigation" data-v-5976474c data-v-aa56eba0><div class="content" data-v-aa56eba0><div class="outline-marker" data-v-aa56eba0></div><div id="doc-outline-aria-label" aria-level="2" class="outline-title" role="heading" data-v-aa56eba0><span data-v-aa56eba0>On this page</span><span class="vpi-print icon" data-v-aa56eba0></span></div><ul class="root" data-v-aa56eba0 data-v-3e6b023c><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-5976474c></div><!--[--><!--]--></div></div></div></div><div class="content" data-v-23f6ad98><div class="content-container" data-v-23f6ad98><!--[--><!--]--><main class="main" data-v-23f6ad98><nav class="vp-breadcrumb" data-v-23f6ad98 data-v-1ae4ad7a><ol vocab="https://schema.org/" typeof="BreadcrumbList" data-v-1ae4ad7a><!--[--><li property="itemListElement" typeof="ListItem" data-v-1ae4ad7a><a class="vp-link link breadcrumb" href="/wan-s-blog/" property="item" typeof="WebPage" data-v-1ae4ad7a><!--[-->Home<!--]--><!----></a><span class="vpi-chevron-right" data-v-1ae4ad7a></span><meta property="name" content="Home" data-v-1ae4ad7a><meta property="position" content="1" data-v-1ae4ad7a></li><li property="itemListElement" typeof="ListItem" data-v-1ae4ad7a><span class="vp-link breadcrumb" property="item" typeof="WebPage" data-v-1ae4ad7a><!--[-->Agents<!--]--><!----></span><span class="vpi-chevron-right" data-v-1ae4ad7a></span><meta property="name" content="Agents" data-v-1ae4ad7a><meta property="position" content="2" data-v-1ae4ad7a></li><li property="itemListElement" typeof="ListItem" data-v-1ae4ad7a><a class="vp-link link breadcrumb current" href="/wan-s-blog/fit3080/pb1j0vwr/" property="item" typeof="WebPage" data-v-1ae4ad7a><!--[-->Intelligent Agents and Rationality<!--]--><!----></a><!----><meta property="name" content="Intelligent Agents and Rationality" data-v-1ae4ad7a><meta property="position" content="3" data-v-1ae4ad7a></li><!--]--></ol></nav><!--[--><!--]--><!--[--><h1 class="vp-doc-title page-title" data-v-27be53cb>Intelligent Agents and Rationality <!----></h1><div class="vp-doc-meta" data-v-27be53cb><!--[--><!--]--><p class="reading-time" data-v-27be53cb><span class="vpi-books icon" data-v-27be53cb></span><span data-v-27be53cb>About 1531 words</span><span data-v-27be53cb>About 19 min</span></p><!----><!--[--><!--]--><p class="create-time" data-v-27be53cb><span class="vpi-clock icon" data-v-27be53cb></span><span data-v-27be53cb>2025-08-03</span></p></div><!--]--><!--[--><!--]--><div class="_fit3080_pb1j0vwr_ external-link-icon-enabled vp-doc plume-content" vp-content data-v-23f6ad98><!--[--><!--]--><div data-v-23f6ad98><h2 id="what-is-an-agent" tabindex="-1"><a class="header-anchor" href="#what-is-an-agent"><span>What is an Agent?</span></a></h2><p>An <strong>agent</strong> is anything that can perceive its environment through <strong>sensors</strong> <span class="vp-annotation ignore-header sensors" data-v-6eb215f5><span aria-label="sensors" class="vpi-annotation" data-v-6eb215f5></span><!----></span> and act upon that environment through <strong>actuators</strong> <span class="vp-annotation ignore-header actuators" data-v-6eb215f5><span aria-label="actuators" class="vpi-annotation" data-v-6eb215f5></span><!----></span>.</p><div class="vp-card-grid cols-1" style="grid-template-columns:repeat(1, 1fr);" data-v-d930ab6f><!--[--><article class="vp-card-wrapper" data-v-0aa8820e><!--[--><p class="title" data-v-0aa8820e><span class="vp-icon vpi-gqds4k38" style="" aria-hidden data-provider="iconify" data-v-0aa8820e></span><span class="text" data-v-0aa8820e>Human Agent</span></p><!--]--><div class="body" data-v-0aa8820e><!--[--><ul><li><strong>Sensors</strong>: Eyes, ears, and other organs.</li><li><strong>Actuators</strong>: Hands, legs, mouth, and other body parts.</li></ul><!--]--></div></article><article class="vp-card-wrapper" data-v-0aa8820e><!--[--><p class="title" data-v-0aa8820e><span class="vp-icon vpi-z0uqvckm" style="" aria-hidden data-provider="iconify" data-v-0aa8820e></span><span class="text" data-v-0aa8820e>Robotic Agent</span></p><!--]--><div class="body" data-v-0aa8820e><!--[--><ul><li><strong>Sensors</strong>: Cameras, infrared range finders.</li><li><strong>Actuators</strong>: Various motors to move arms, wheels, etc.</li></ul><!--]--></div></article><article class="vp-card-wrapper" data-v-0aa8820e><!--[--><p class="title" data-v-0aa8820e><span class="vp-icon vpi-oa6b7qqi" style="" aria-hidden data-provider="iconify" data-v-0aa8820e></span><span class="text" data-v-0aa8820e>Software Agent</span></p><!--]--><div class="body" data-v-0aa8820e><!--[--><ul><li><strong>Sensors</strong>: Keyboard input, file contents, network packets.</li><li><strong>Actuators</strong>: Displaying on screen, writing to a file, sending network packets.</li></ul><!--]--></div></article><!--]--></div><h3 id="the-agent-environment-interaction" tabindex="-1"><a class="header-anchor" href="#the-agent-environment-interaction"><span>The Agent-Environment Interaction</span></a></h3><p>An agent operates in a continuous loop: it receives <strong>percepts</strong> <span class="vp-annotation ignore-header percepts" data-v-6eb215f5><span aria-label="percepts" class="vpi-annotation" data-v-6eb215f5></span><!----></span> from the environment and performs <strong>actions</strong> that change the environment.</p><ul><li><p><strong>Agent Function</strong> <span class="vp-annotation ignore-header agent-function" data-v-6eb215f5><span aria-label="agent-function" class="vpi-annotation" data-v-6eb215f5></span><!----></span>: This is an abstract mathematical mapping from a history of percepts to an action. It&#39;s represented as:</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>f</mi><mo>:</mo><msup><mi mathvariant="script">P</mi><mo>∗</mo></msup><mo>→</mo><mi mathvariant="script">A</mi></mrow><annotation encoding="application/x-tex">f: \mathcal{P}^* \rightarrow \mathcal{A} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7387em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.08222em;">P</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7387em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathcal">A</span></span></span></span></span></p><p>Where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="script">P</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">\mathcal{P}^*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6887em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.08222em;">P</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6887em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span></span></span></span> is the sequence of all percepts, and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">A</mi></mrow><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathcal">A</span></span></span></span> is the set of all possible actions.</p></li><li><p><strong>Agent Program</strong> <span class="vp-annotation ignore-header agent-program" data-v-6eb215f5><span aria-label="agent-program" class="vpi-annotation" data-v-6eb215f5></span><!----></span>: This is the concrete implementation of the agent function. It&#39;s the actual code that runs on the agent&#39;s physical architecture.</p><blockquote><p><code>Agent = Architecture + Program</code></p></blockquote></li></ul><h2 id="rationality" tabindex="-1"><a class="header-anchor" href="#rationality"><span>Rationality</span></a></h2><p>A <strong>rational agent</strong> is one that does the &quot;right thing.&quot; But what does &quot;right&quot; mean?</p><details class="hint-container details"><summary>Defining Rationality</summary><p>For every possible sequence of percepts, a rational agent should choose an action that is expected to <mark class="tip">maximize its performance measure</mark>, given the evidence from the percepts and any built-in knowledge the agent has.</p></details><h3 id="key-aspects-of-a-rational-agent" tabindex="-1"><a class="header-anchor" href="#key-aspects-of-a-rational-agent"><span>Key Aspects of a Rational Agent</span></a></h3><p>When an agent decides how to act, its choice is guided by four factors:</p><ol><li><strong>Performance Measure</strong>: A well-defined metric to evaluate success.</li><li><strong>Prior Knowledge</strong>: The agent&#39;s built-in understanding of the environment.</li><li><strong>Available Actions</strong>: The set of all possible actions the agent can perform.</li><li><strong>Percept History</strong>: All the information the agent has gathered from the environment so far.</li></ol><div class="hint-container important"><p class="hint-container-title">Rationality vs. Omniscience</p><p>Rationality is <strong>not</strong> the same as <strong>omniscience</strong> <span class="vp-annotation ignore-header omniscience" data-v-6eb215f5><span aria-label="omniscience" class="vpi-annotation" data-v-6eb215f5></span><!----></span>. An omniscient agent knows the <em>actual</em> outcome of its actions and can see the future. A rational agent simply makes the best decision based on what it knows <em>now</em>.</p><ul><li>A perfectly rational poker agent can&#39;t know what cards the opponent holds, so it can still lose a hand. Its rationality is based on making the play that maximizes its expected winnings over the long run.</li></ul></div><h3 id="autonomy-and-learning" tabindex="-1"><a class="header-anchor" href="#autonomy-and-learning"><span>Autonomy and Learning</span></a></h3><ul><li><strong>Information Gathering</strong>: A rational agent may perform actions to explore its environment and gather more information. This is crucial for making better decisions later.</li><li><strong>Autonomy</strong> <span class="vp-annotation ignore-header autonomy" data-v-6eb215f5><span aria-label="autonomy" class="vpi-annotation" data-v-6eb215f5></span><!----></span>: An agent is autonomous if it learns from its own experiences to improve its behavior, rather than relying solely on the initial knowledge provided by its designer.</li></ul><h2 id="task-environments-peas" tabindex="-1"><a class="header-anchor" href="#task-environments-peas"><span>Task Environments (PEAS)</span></a></h2><p>To design a rational agent, we must first specify its task environment. The <strong>PEAS</strong> <span class="vp-annotation ignore-header peas" data-v-6eb215f5><span aria-label="peas" class="vpi-annotation" data-v-6eb215f5></span><!----></span> framework is used for this purpose.</p><div class="vp-field-group"><div class="vp-field required"><p class="field-meta"><span class="name">P</span><span class="required">Required</span><!----><span class="type"><code>Performance Measure</code></span></p><!----><div class="description"><!--[--><p>How is success measured? What do we want the agent to achieve?</p><!--]--></div></div><div class="vp-field required"><p class="field-meta"><span class="name">E</span><span class="required">Required</span><!----><span class="type"><code>Environment</code></span></p><!----><div class="description"><!--[--><p>Where does the agent operate? What are the &quot;laws of physics&quot; of this world?</p><!--]--></div></div><div class="vp-field required"><p class="field-meta"><span class="name">A</span><span class="required">Required</span><!----><span class="type"><code>Actuators</code></span></p><!----><div class="description"><!--[--><p>What can the agent <em>do</em>? How does it affect the environment?</p><!--]--></div></div><div class="vp-field required"><p class="field-meta"><span class="name">S</span><span class="required">Required</span><!----><span class="type"><code>Sensors</code></span></p><!----><div class="description"><!--[--><p>How does the agent <em>perceive</em> the environment? What information can it gather?</p><!--]--></div></div></div><h3 id="peas-example-automated-taxi" tabindex="-1"><a class="header-anchor" href="#peas-example-automated-taxi"><span>PEAS Example: Automated Taxi</span></a></h3><div class="vp-collapse"><!--[--><div class="vp-collapse-item"><div class="vp-collapse-header"><span class="vpi-chevron-right"></span><p class="vp-collapse-title"><!--[--><strong>Performance Measure</strong><!--]--></p></div><div class="vp-collapse-content" style="display:none;"><div class="vp-collapse-content-inner"><!--[--><ul><li>Safety (e.g., +500 for arriving unharmed, -100 for a crash).</li><li>Speed (e.g., -0.1 points per second).</li><li>Legality (e.g., -500 per violation).</li><li>Comfort (e.g., -10 for speed bumps, -50 for sudden turns).</li><li>Profit (e.g., maximize fare, minimize fuel).</li></ul><!--]--></div></div></div><div class="vp-collapse-item"><div class="vp-collapse-header"><span class="vpi-chevron-right"></span><p class="vp-collapse-title"><!--[--><strong>Environment</strong><!--]--></p></div><div class="vp-collapse-content" style="display:none;"><div class="vp-collapse-content-inner"><!--[--><ul><li>Roads, traffic, pedestrians, customers, weather.</li></ul><!--]--></div></div></div><div class="vp-collapse-item"><div class="vp-collapse-header"><span class="vpi-chevron-right"></span><p class="vp-collapse-title"><!--[--><strong>Actuators</strong><!--]--></p></div><div class="vp-collapse-content" style="display:none;"><div class="vp-collapse-content-inner"><!--[--><ul><li>Steering, accelerator, brake, turn signals, horn, display screens.</li></ul><!--]--></div></div></div><div class="vp-collapse-item"><div class="vp-collapse-header"><span class="vpi-chevron-right"></span><p class="vp-collapse-title"><!--[--><strong>Sensors</strong><!--]--></p></div><div class="vp-collapse-content" style="display:none;"><div class="vp-collapse-content-inner"><!--[--><ul><li>Cameras, sonar, GPS, speedometer, odometer, engine sensors, microphone for speech recognition.</li></ul><!--]--></div></div></div><!--]--></div><h2 id="environment-types" tabindex="-1"><a class="header-anchor" href="#environment-types"><span>Environment Types</span></a></h2><p>The design of an agent is heavily influenced by the type of environment it operates in.</p><div class="vp-tabs"><div class="vp-tabs-nav" role="tablist"><!--[--><button class="active vp-tab-nav" type="button" role="tab" aria-controls="tab-252-0" aria-selected="true"><!--[-->Observability &amp; Knowledge<!--]--></button><button class="vp-tab-nav" type="button" role="tab" aria-controls="tab-252-1" aria-selected="false"><!--[-->Agency &amp; Determinism<!--]--></button><button class="vp-tab-nav" type="button" role="tab" aria-controls="tab-252-2" aria-selected="false"><!--[-->Time &amp; State<!--]--></button><!--]--></div><!--[--><div id="tab-252-0" class="active vp-tab" role="tabpanel" aria-expanded="true"><div class="vp-tab-title"><!--[-->Observability &amp; Knowledge<!--]--></div><!--[--><ul><li><strong>Fully Observable vs. Partially Observable</strong>: Can the agent&#39;s sensors detect all aspects of the environment&#39;s state that are relevant to the choice of action? If not, it&#39;s partially observable.</li><li><strong>Known vs. Unknown</strong>: Does the agent understand the &quot;rules&quot; of the environment? For example, does it know what its actions will do and how the world evolves?</li></ul><!--]--></div><div id="tab-252-1" class="vp-tab" role="tabpanel" aria-expanded="false"><div class="vp-tab-title"><!--[-->Agency &amp; Determinism<!--]--></div><!--[--><ul><li><strong>Single-agent vs. Multi-agent</strong>: Is the agent operating by itself, or are there other agents in the environment whose actions affect the outcome?</li><li><strong>Deterministic vs. Stochastic</strong> <span class="vp-annotation ignore-header stochastic" data-v-6eb215f5><span aria-label="stochastic" class="vpi-annotation" data-v-6eb215f5></span><!----></span>: Is the next state of the environment completely determined by the current state and the agent&#39;s action? If there&#39;s an element of randomness, it&#39;s stochastic.</li></ul><!--]--></div><div id="tab-252-2" class="vp-tab" role="tabpanel" aria-expanded="false"><div class="vp-tab-title"><!--[-->Time &amp; State<!--]--></div><!--[--><ul><li><strong>Episodic vs. Sequential</strong> <span class="vp-annotation ignore-header episodic" data-v-6eb215f5><span aria-label="episodic" class="vpi-annotation" data-v-6eb215f5></span><!----></span>: Is the agent&#39;s experience divided into independent, atomic episodes (like an image classifier identifying single images)? Or do past actions affect future decisions (like in a game of chess)?</li><li><strong>Static vs. Dynamic</strong>: Can the environment change while the agent is deliberating? A dynamic environment is more challenging as it &quot;punishes&quot; slow agents.</li><li><strong>Discrete vs. Continuous</strong>: Does the environment have a finite or infinite number of distinct states? This also applies to time, percepts, and actions.</li></ul><!--]--></div><!--]--></div><h2 id="agent-types" tabindex="-1"><a class="header-anchor" href="#agent-types"><span>Agent Types</span></a></h2><p>Agents can be categorized based on the complexity of their decision-making process.</p><div class="vp-timeline"><div class="vp-timeline-box"><!--[--><div class="vp-timeline-item card info line-solid placement-left" style=""><div class="has-icon vp-timeline-line"><span class="vp-timeline-point"><!--[--><span class="vp-icon vpi-xoj3b3go" style="" aria-hidden data-provider="iconify"></span><!--]--></span></div><div class="vp-timeline-container"><div class="vp-timeline-content"><p class="vp-timeline-title"><!--[-->Simple Reflex Agent<!--]--></p><!--[--><p><strong>Action basis</strong>: Current percept only. It ignores the rest of the percept history and operates on simple <code>condition-action</code> rules.<br><strong>Example</strong>: A car that brakes <em>only</em> when the brake lights of the car in front light up.</p><!--]--></div><p class="vp-timeline-time">Level 1</p></div></div><div class="vp-timeline-item card tip line-solid placement-left" style=""><div class="has-icon vp-timeline-line"><span class="vp-timeline-point"><!--[--><span class="vp-icon vpi-3gruym3h" style="" aria-hidden data-provider="iconify"></span><!--]--></span></div><div class="vp-timeline-container"><div class="vp-timeline-content"><p class="vp-timeline-title"><!--[-->Model-based Reflex Agent<!--]--></p><!--[--><p><strong>Action basis</strong>: Internal state, which models how the world works. This agent maintains an internal model of the world to handle partial observability. It tracks the state of things it can&#39;t currently see.<br><strong>Example</strong>: A taxi agent remembering which roads it has already traveled.</p><!--]--></div><p class="vp-timeline-time">Level 2</p></div></div><div class="vp-timeline-item card success line-solid placement-left" style=""><div class="has-icon vp-timeline-line"><span class="vp-timeline-point"><!--[--><span class="vp-icon vpi-mdfn6l2l" style="" aria-hidden data-provider="iconify"></span><!--]--></span></div><div class="vp-timeline-container"><div class="vp-timeline-content"><p class="vp-timeline-title"><!--[-->Goal-based Agent<!--]--></p><!--[--><p><strong>Action basis</strong>: Model-based + explicit goals. This agent considers future outcomes. It asks, &quot;Which of my possible actions will lead me to a goal state?&quot;<br><strong>Example</strong>: A taxi agent formulating a plan (a sequence of turns) to reach a specific destination.</p><!--]--></div><p class="vp-timeline-time">Level 3</p></div></div><div class="vp-timeline-item card warning line-solid placement-left" style=""><div class="has-icon vp-timeline-line"><span class="vp-timeline-point"><!--[--><span class="vp-icon vpi-cootgidg" style="" aria-hidden data-provider="iconify"></span><!--]--></span></div><div class="vp-timeline-container"><div class="vp-timeline-content"><p class="vp-timeline-title"><!--[-->Utility-based Agent<!--]--></p><!--[--><p><strong>Action basis</strong>: Model-based + a utility function. When there are multiple paths to a goal, this agent chooses the one that maximizes its &quot;happiness&quot; or utility. Utility is a function that maps a state to a real number representing its desirability.<br><strong>Example</strong>: A taxi agent choosing the route that is not just correct, but is also the quickest and most fuel-efficient.</p><!--]--></div><p class="vp-timeline-time">Level 4</p></div></div><div class="vp-timeline-item card danger line-solid placement-left" style=""><div class="has-icon vp-timeline-line"><span class="vp-timeline-point"><!--[--><span class="vp-icon vpi-iclqo80l" style="" aria-hidden data-provider="iconify"></span><!--]--></span></div><div class="vp-timeline-container"><div class="vp-timeline-content"><p class="vp-timeline-title"><!--[-->Learning Agent<!--]--></p><!--[--><p><strong>Action basis</strong>: All of the above, plus the ability to improve. A learning agent can operate in unknown environments and become more competent than its initial knowledge allows. It has four main components:</p><p><strong>Performance Element</strong>: The agent itself (e.g., a utility-based agent).<br><strong>Critic</strong>: Provides feedback on how well the agent is doing.<br><strong>Learning Element</strong>: Uses feedback to modify the performance element.<br><strong>Problem Generator</strong>: Suggests exploratory actions to gain new, informative experiences.</p><!--]--></div><p class="vp-timeline-time">Level 5</p></div></div><!--]--></div></div></div><!----><!--[--><h2 id="doc-changelog" tabindex="-1"><a href="#doc-changelog" class="header-anchor"><span>Changelog</span></a></h2><div class="vp-changelog-wrapper"><div class="vp-changelog-header"><div class="vp-latest-updated"><span class="vp-changelog-icon"></span><span data-allow-mismatch>8/8/25, 1:50 PM</span></div><div><span class="vp-changelog-menu-icon"></span><span>View All Changelog</span></div></div><ul class="vp-changelog-list"><!--[--><li class="vp-changelog-item-commit"><span class="vp-changelog-hash" target="_blank" rel="noreferrer"><code>22b76</code></span><span class="vp-changelog-divider">-</span><span class="vp-changelog-message">feat: drop section for agents</span><span class="vp-changelog-date" data-allow-mismatch>on <time datetime="2025-08-08T13:50:56.000Z">8/8/25</time></span></li><!--]--></ul></div><!--]--><div class="vp-doc-copyright" data-v-23f6ad98><h2 id="doc-copyright" tabindex="-1" class="vp-doc-header" data-v-309da98c><a href="#doc-copyright" class="header-anchor" data-v-309da98c><span data-v-309da98c><!--[-->Copyright<!--]--></span></a></h2><div class="hint-container tip copyright-container" data-v-22c67d2a><p data-v-22c67d2a><span data-v-22c67d2a>Copyright Ownership:</span><a class="vp-link link no-icon" href="https://github.com/yunfenglong" target="_blank" rel="noreferrer" data-v-22c67d2a><!--[-->WARREN Y.F. LONG<!--]--><!----></a></p><!----><p data-v-22c67d2a><span data-v-22c67d2a>License under:</span><a class="vp-link link no-icon" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" rel="noreferrer" data-v-22c67d2a><!--[-->Attribution-NonCommercial-NoDerivatives 4.0 International (CC-BY-NC-ND-4.0)<!--]--><!----></a><!--[--><span class="vpi-license-cc" data-v-22c67d2a></span><span class="vpi-license-by" data-v-22c67d2a></span><span class="vpi-license-nc" data-v-22c67d2a></span><span class="vpi-license-nd" data-v-22c67d2a></span><!--]--></p></div></div></div></main><footer class="vp-doc-footer" data-v-23f6ad98 data-v-fda6bbae><!--[--><!--]--><!----><!----><nav class="prev-next" data-v-fda6bbae><div class="pager" data-v-fda6bbae><a class="vp-link link pager-link prev" href="/wan-s-blog/fit3080/94l7fkfk/" data-v-fda6bbae><!--[--><span class="desc" data-v-fda6bbae>Previous page</span><span class="title" data-v-fda6bbae>Intro to AI</span><!--]--><!----></a></div><div class="pager" data-v-fda6bbae><a class="vp-link link pager-link next" href="/wan-s-blog/fit3080/1tgvq8kr/" data-v-fda6bbae><!--[--><span class="desc" data-v-fda6bbae>Next page</span><span class="title" data-v-fda6bbae>Real World Implementation</span><!--]--><!----></a></div></nav></footer><!----><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!--]--><button style="display:none;" type="button" class="vp-back-to-top" aria-label="back to top" data-v-d90a7a26 data-v-bcf8d9a6><span class="percent" data-allow-mismatch data-v-bcf8d9a6>0%</span><span class="show icon vpi-back-to-top" data-v-bcf8d9a6></span><svg aria-hidden="true" data-v-bcf8d9a6><circle cx="50%" cy="50%" data-allow-mismatch style="stroke-dasharray:calc(0% - 12.566370614359172px) calc(314.1592653589793% - 12.566370614359172px);" data-v-bcf8d9a6></circle></svg></button><footer class="vp-footer has-sidebar" vp-footer data-v-d90a7a26 data-v-400675cf><!--[--><div class="container" data-v-400675cf><p class="message" data-v-400675cf>Power by <a target="_blank" href="https://v2.vuepress.vuejs.org/">VuePress</a> & <a target="_blank" href="https://theme-plume.vuejs.press">vuepress-theme-plume</a></p><!----></div><!--]--></footer><!--[--><!--]--><!--]--></div><!----><!--]--><!--[--><!--]--><!--]--></div><script type="module" src="/wan-s-blog/assets/app-C7AwxLk6.js" defer></script></body></html>