import{a as w,c as A,d as n,e as a,b as f,f as e,w as s,r as l,o as c}from"./app-C7AwxLk6.js";const x={},k={class:"hint-container important"},P={class:"vp-field-group"};function T(I,t){const i=l("Annotation"),g=l("VPCard"),h=l("VPCardGrid"),u=l("VPField"),d=l("VPCollapseItem"),b=l("VPCollapse"),v=l("Tabs"),o=l("VPIcon"),r=l("VPTimelineItem"),y=l("VPTimeline");return c(),A("div",null,[t[92]||(t[92]=n("h2",{id:"what-is-an-agent",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#what-is-an-agent"},[n("span",null,"What is an Agent?")])],-1)),n("p",null,[t[2]||(t[2]=e("An ",-1)),t[3]||(t[3]=n("strong",null,"agent",-1)),t[4]||(t[4]=e(" is anything that can perceive its environment through ",-1)),t[5]||(t[5]=n("strong",null,"sensors",-1)),t[6]||(t[6]=e()),a(i,{label:"sensors",total:1},{"item-0":s(()=>t[0]||(t[0]=[n("p",null,[n("strong",null,"Sensors"),e(" are the input mechanisms that allow an agent to perceive its environment. They collect information about the current state of the world and convert it into a form the agent can process.")],-1)])),_:1}),t[7]||(t[7]=e(" and act upon that environment through ",-1)),t[8]||(t[8]=n("strong",null,"actuators",-1)),t[9]||(t[9]=e()),a(i,{label:"actuators",total:1},{"item-0":s(()=>t[1]||(t[1]=[n("p",null,[n("strong",null,"Actuators"),e(` are the output mechanisms that allow an agent to act upon its environment. They are the "effectors" that execute the agent's decisions and cause changes in the world.`)],-1)])),_:1}),t[10]||(t[10]=e(".",-1))]),a(h,null,{default:s(()=>[a(g,{title:"Human Agent",icon:"mdi:human"},{default:s(()=>t[11]||(t[11]=[n("ul",null,[n("li",null,[n("strong",null,"Sensors"),e(": Eyes, ears, and other organs.")]),n("li",null,[n("strong",null,"Actuators"),e(": Hands, legs, mouth, and other body parts.")])],-1)])),_:1,__:[11]}),a(g,{title:"Robotic Agent",icon:"mdi:robot"},{default:s(()=>t[12]||(t[12]=[n("ul",null,[n("li",null,[n("strong",null,"Sensors"),e(": Cameras, infrared range finders.")]),n("li",null,[n("strong",null,"Actuators"),e(": Various motors to move arms, wheels, etc.")])],-1)])),_:1,__:[12]}),a(g,{title:"Software Agent",icon:"mdi:monitor"},{default:s(()=>t[13]||(t[13]=[n("ul",null,[n("li",null,[n("strong",null,"Sensors"),e(": Keyboard input, file contents, network packets.")]),n("li",null,[n("strong",null,"Actuators"),e(": Displaying on screen, writing to a file, sending network packets.")])],-1)])),_:1,__:[13]})]),_:1}),t[93]||(t[93]=n("h3",{id:"the-agent-environment-interaction",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#the-agent-environment-interaction"},[n("span",null,"The Agent-Environment Interaction")])],-1)),n("p",null,[t[15]||(t[15]=e("An agent operates in a continuous loop: it receives ",-1)),t[16]||(t[16]=n("strong",null,"percepts",-1)),t[17]||(t[17]=e()),a(i,{label:"percepts",total:1},{"item-0":s(()=>t[14]||(t[14]=[n("p",null,[n("strong",null,"Percepts"),e(" are the individual sensory inputs that an agent receives at any given moment. They represent the agent's immediate perception of the environment's current state, as detected by its sensors.")],-1)])),_:1}),t[18]||(t[18]=e(" from the environment and performs ",-1)),t[19]||(t[19]=n("strong",null,"actions",-1)),t[20]||(t[20]=e(" that change the environment.",-1))]),n("ul",null,[n("li",null,[n("p",null,[t[22]||(t[22]=n("strong",null,"Agent Function",-1)),t[23]||(t[23]=e()),a(i,{label:"agent-function",total:1},{"item-0":s(()=>t[21]||(t[21]=[n("p",null,[e("The "),n("strong",null,"Agent Function"),e(" is a mathematical abstraction that defines the ideal behavior of an agent. It maps every possible sequence of percepts to the optimal action the agent should take in that situation.")],-1)])),_:1}),t[24]||(t[24]=e(": This is an abstract mathematical mapping from a history of percepts to an action. It's represented as:",-1))]),t[25]||(t[25]=f('<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>f</mi><mo>:</mo><msup><mi mathvariant="script">P</mi><mo>∗</mo></msup><mo>→</mo><mi mathvariant="script">A</mi></mrow><annotation encoding="application/x-tex">f: \\mathcal{P}^* \\rightarrow \\mathcal{A} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7387em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.08222em;">P</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7387em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathcal">A</span></span></span></span></span></p><p>Where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="script">P</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">\\mathcal{P}^*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6887em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.08222em;">P</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6887em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span></span></span></span> is the sequence of all percepts, and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">A</mi></mrow><annotation encoding="application/x-tex">\\mathcal{A}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathcal">A</span></span></span></span> is the set of all possible actions.</p>',2))]),n("li",null,[n("p",null,[t[27]||(t[27]=n("strong",null,"Agent Program",-1)),t[28]||(t[28]=e()),a(i,{label:"agent-program",total:1},{"item-0":s(()=>t[26]||(t[26]=[n("p",null,[e("The "),n("strong",null,"Agent Program"),e(" is the concrete software implementation that actually runs on the agent's hardware. It approximates the agent function within the constraints of real-world computing resources.")],-1)])),_:1}),t[29]||(t[29]=e(": This is the concrete implementation of the agent function. It's the actual code that runs on the agent's physical architecture.",-1))]),t[30]||(t[30]=n("blockquote",null,[n("p",null,[n("code",null,"Agent = Architecture + Program")])],-1))])]),t[94]||(t[94]=f('<h2 id="rationality" tabindex="-1"><a class="header-anchor" href="#rationality"><span>Rationality</span></a></h2><p>A <strong>rational agent</strong> is one that does the &quot;right thing.&quot; But what does &quot;right&quot; mean?</p><details class="hint-container details"><summary>Defining Rationality</summary><p>For every possible sequence of percepts, a rational agent should choose an action that is expected to <mark class="tip">maximize its performance measure</mark>, given the evidence from the percepts and any built-in knowledge the agent has.</p></details><h3 id="key-aspects-of-a-rational-agent" tabindex="-1"><a class="header-anchor" href="#key-aspects-of-a-rational-agent"><span>Key Aspects of a Rational Agent</span></a></h3><p>When an agent decides how to act, its choice is guided by four factors:</p><ol><li><strong>Performance Measure</strong>: A well-defined metric to evaluate success.</li><li><strong>Prior Knowledge</strong>: The agent&#39;s built-in understanding of the environment.</li><li><strong>Available Actions</strong>: The set of all possible actions the agent can perform.</li><li><strong>Percept History</strong>: All the information the agent has gathered from the environment so far.</li></ol>',6)),n("div",k,[t[42]||(t[42]=n("p",{class:"hint-container-title"},"Rationality vs. Omniscience",-1)),n("p",null,[t[32]||(t[32]=e("Rationality is ",-1)),t[33]||(t[33]=n("strong",null,"not",-1)),t[34]||(t[34]=e(" the same as ",-1)),t[35]||(t[35]=n("strong",null,"omniscience",-1)),t[36]||(t[36]=e()),a(i,{label:"omniscience",total:1},{"item-0":s(()=>t[31]||(t[31]=[n("p",null,[n("strong",null,"Omniscience"),e(" refers to having complete knowledge of all facts, including future outcomes. An omniscient agent would know the exact results of all possible actions before taking them, which is impossible for real-world agents.")],-1)])),_:1}),t[37]||(t[37]=e(". An omniscient agent knows the ",-1)),t[38]||(t[38]=n("em",null,"actual",-1)),t[39]||(t[39]=e(" outcome of its actions and can see the future. A rational agent simply makes the best decision based on what it knows ",-1)),t[40]||(t[40]=n("em",null,"now",-1)),t[41]||(t[41]=e(".",-1))]),t[43]||(t[43]=n("ul",null,[n("li",null,"A perfectly rational poker agent can't know what cards the opponent holds, so it can still lose a hand. Its rationality is based on making the play that maximizes its expected winnings over the long run.")],-1))]),t[95]||(t[95]=n("h3",{id:"autonomy-and-learning",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#autonomy-and-learning"},[n("span",null,"Autonomy and Learning")])],-1)),n("ul",null,[t[48]||(t[48]=n("li",null,[n("strong",null,"Information Gathering"),e(": A rational agent may perform actions to explore its environment and gather more information. This is crucial for making better decisions later.")],-1)),n("li",null,[t[45]||(t[45]=n("strong",null,"Autonomy",-1)),t[46]||(t[46]=e()),a(i,{label:"autonomy",total:1},{"item-0":s(()=>t[44]||(t[44]=[n("p",null,[n("strong",null,"Autonomy"),e(" in AI refers to an agent's ability to adapt and improve its performance through learning from experience, rather than being completely dependent on pre-programmed knowledge from its designer.")],-1)])),_:1}),t[47]||(t[47]=e(": An agent is autonomous if it learns from its own experiences to improve its behavior, rather than relying solely on the initial knowledge provided by its designer.",-1))])]),t[96]||(t[96]=n("h2",{id:"task-environments-peas",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#task-environments-peas"},[n("span",null,"Task Environments (PEAS)")])],-1)),n("p",null,[t[50]||(t[50]=e("To design a rational agent, we must first specify its task environment. The ",-1)),t[51]||(t[51]=n("strong",null,"PEAS",-1)),t[52]||(t[52]=e()),a(i,{label:"peas",total:1},{"item-0":s(()=>t[49]||(t[49]=[n("p",null,[n("strong",null,"PEAS"),e(" stands for "),n("strong",null,"Performance measure, Environment, Actuators, Sensors"),e(". It's a framework for completely specifying a task environment, ensuring that all relevant aspects of the agent's operating context are considered during design.")],-1)])),_:1}),t[53]||(t[53]=e(" framework is used for this purpose.",-1))]),n("div",P,[a(u,{name:"P",required:"",type:"Performance Measure"},{default:s(()=>t[54]||(t[54]=[n("p",null,"How is success measured? What do we want the agent to achieve?",-1)])),_:1,__:[54]}),a(u,{name:"E",required:"",type:"Environment"},{default:s(()=>t[55]||(t[55]=[n("p",null,'Where does the agent operate? What are the "laws of physics" of this world?',-1)])),_:1,__:[55]}),a(u,{name:"A",required:"",type:"Actuators"},{default:s(()=>t[56]||(t[56]=[n("p",null,[e("What can the agent "),n("em",null,"do"),e("? How does it affect the environment?")],-1)])),_:1,__:[56]}),a(u,{name:"S",required:"",type:"Sensors"},{default:s(()=>t[57]||(t[57]=[n("p",null,[e("How does the agent "),n("em",null,"perceive"),e(" the environment? What information can it gather?")],-1)])),_:1,__:[57]})]),t[97]||(t[97]=n("h3",{id:"peas-example-automated-taxi",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#peas-example-automated-taxi"},[n("span",null,"PEAS Example: Automated Taxi")])],-1)),a(b,null,{default:s(()=>[a(d,{index:0},{title:s(()=>t[58]||(t[58]=[n("strong",null,"Performance Measure",-1)])),default:s(()=>[t[59]||(t[59]=n("ul",null,[n("li",null,"Safety (e.g., +500 for arriving unharmed, -100 for a crash)."),n("li",null,"Speed (e.g., -0.1 points per second)."),n("li",null,"Legality (e.g., -500 per violation)."),n("li",null,"Comfort (e.g., -10 for speed bumps, -50 for sudden turns)."),n("li",null,"Profit (e.g., maximize fare, minimize fuel).")],-1))]),_:1,__:[59]}),a(d,{index:1},{title:s(()=>t[60]||(t[60]=[n("strong",null,"Environment",-1)])),default:s(()=>[t[61]||(t[61]=n("ul",null,[n("li",null,"Roads, traffic, pedestrians, customers, weather.")],-1))]),_:1,__:[61]}),a(d,{index:2},{title:s(()=>t[62]||(t[62]=[n("strong",null,"Actuators",-1)])),default:s(()=>[t[63]||(t[63]=n("ul",null,[n("li",null,"Steering, accelerator, brake, turn signals, horn, display screens.")],-1))]),_:1,__:[63]}),a(d,{index:3},{title:s(()=>t[64]||(t[64]=[n("strong",null,"Sensors",-1)])),default:s(()=>[t[65]||(t[65]=n("ul",null,[n("li",null,"Cameras, sonar, GPS, speedometer, odometer, engine sensors, microphone for speech recognition.")],-1))]),_:1,__:[65]})]),_:1}),t[98]||(t[98]=n("h2",{id:"environment-types",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#environment-types"},[n("span",null,"Environment Types")])],-1)),t[99]||(t[99]=n("p",null,"The design of an agent is heavily influenced by the type of environment it operates in.",-1)),a(v,{id:"252",data:[{id:"Observability & Knowledge"},{id:"Agency & Determinism"},{id:"Time & State"}]},{title0:s(({value:m,isActive:p})=>t[66]||(t[66]=[e("Observability & Knowledge",-1)])),title1:s(({value:m,isActive:p})=>t[67]||(t[67]=[e("Agency & Determinism",-1)])),title2:s(({value:m,isActive:p})=>t[68]||(t[68]=[e("Time & State",-1)])),tab0:s(({value:m,isActive:p})=>t[69]||(t[69]=[n("ul",null,[n("li",null,[n("strong",null,"Fully Observable vs. Partially Observable"),e(": Can the agent's sensors detect all aspects of the environment's state that are relevant to the choice of action? If not, it's partially observable.")]),n("li",null,[n("strong",null,"Known vs. Unknown"),e(': Does the agent understand the "rules" of the environment? For example, does it know what its actions will do and how the world evolves?')])],-1)])),tab1:s(({value:m,isActive:p})=>[n("ul",null,[t[74]||(t[74]=n("li",null,[n("strong",null,"Single-agent vs. Multi-agent"),e(": Is the agent operating by itself, or are there other agents in the environment whose actions affect the outcome?")],-1)),n("li",null,[t[71]||(t[71]=n("strong",null,"Deterministic vs. Stochastic",-1)),t[72]||(t[72]=e()),a(i,{label:"stochastic",total:1},{"item-0":s(()=>t[70]||(t[70]=[n("p",null,[e("A "),n("strong",null,"stochastic"),e(" environment is one where the outcome of actions involves randomness or uncertainty. Even with the same initial state and action, different results may occur due to probabilistic factors in the environment.")],-1)])),_:1}),t[73]||(t[73]=e(": Is the next state of the environment completely determined by the current state and the agent's action? If there's an element of randomness, it's stochastic.",-1))])])]),tab2:s(({value:m,isActive:p})=>[n("ul",null,[n("li",null,[t[76]||(t[76]=n("strong",null,"Episodic vs. Sequential",-1)),t[77]||(t[77]=e()),a(i,{label:"episodic",total:1},{"item-0":s(()=>t[75]||(t[75]=[n("p",null,[n("strong",null,"Episodic"),e(" environments divide the agent's experience into independent episodes, where each episode is atomic and doesn't affect future episodes. In contrast, "),n("strong",null,"sequential"),e(" environments have actions that influence future decisions.")],-1)])),_:1}),t[78]||(t[78]=e(": Is the agent's experience divided into independent, atomic episodes (like an image classifier identifying single images)? Or do past actions affect future decisions (like in a game of chess)?",-1))]),t[79]||(t[79]=n("li",null,[n("strong",null,"Static vs. Dynamic"),e(': Can the environment change while the agent is deliberating? A dynamic environment is more challenging as it "punishes" slow agents.')],-1)),t[80]||(t[80]=n("li",null,[n("strong",null,"Discrete vs. Continuous"),e(": Does the environment have a finite or infinite number of distinct states? This also applies to time, percepts, and actions.")],-1))])]),_:1}),t[100]||(t[100]=n("h2",{id:"agent-types",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#agent-types"},[n("span",null,"Agent Types")])],-1)),t[101]||(t[101]=n("p",null,"Agents can be categorized based on the complexity of their decision-making process.",-1)),a(y,{card:""},{default:s(()=>[a(r,{time:"Level 1",type:"info",icon:"carbon:circle-solid",card:void 0},{icon:s(()=>[a(o,{provider:"iconify",name:"carbon:circle-solid"})]),title:s(()=>t[81]||(t[81]=[e("Simple Reflex Agent",-1)])),default:s(()=>[t[82]||(t[82]=n("p",null,[n("strong",null,"Action basis"),e(": Current percept only. It ignores the rest of the percept history and operates on simple "),n("code",null,"condition-action"),e(" rules."),n("br"),n("strong",null,"Example"),e(": A car that brakes "),n("em",null,"only"),e(" when the brake lights of the car in front light up.")],-1))]),_:1,__:[82]}),a(r,{time:"Level 2",type:"tip",icon:"carbon:data-base",card:void 0},{icon:s(()=>[a(o,{provider:"iconify",name:"carbon:data-base"})]),title:s(()=>t[83]||(t[83]=[e("Model-based Reflex Agent",-1)])),default:s(()=>[t[84]||(t[84]=n("p",null,[n("strong",null,"Action basis"),e(": Internal state, which models how the world works. This agent maintains an internal model of the world to handle partial observability. It tracks the state of things it can't currently see."),n("br"),n("strong",null,"Example"),e(": A taxi agent remembering which roads it has already traveled.")],-1))]),_:1,__:[84]}),a(r,{time:"Level 3",type:"success",icon:"mdi:bullseye",card:void 0},{icon:s(()=>[a(o,{provider:"iconify",name:"mdi:bullseye"})]),title:s(()=>t[85]||(t[85]=[e("Goal-based Agent",-1)])),default:s(()=>[t[86]||(t[86]=n("p",null,[n("strong",null,"Action basis"),e(': Model-based + explicit goals. This agent considers future outcomes. It asks, "Which of my possible actions will lead me to a goal state?"'),n("br"),n("strong",null,"Example"),e(": A taxi agent formulating a plan (a sequence of turns) to reach a specific destination.")],-1))]),_:1,__:[86]}),a(r,{time:"Level 4",type:"warning",icon:"carbon:analytics",card:void 0},{icon:s(()=>[a(o,{provider:"iconify",name:"carbon:analytics"})]),title:s(()=>t[87]||(t[87]=[e("Utility-based Agent",-1)])),default:s(()=>[t[88]||(t[88]=n("p",null,[n("strong",null,"Action basis"),e(': Model-based + a utility function. When there are multiple paths to a goal, this agent chooses the one that maximizes its "happiness" or utility. Utility is a function that maps a state to a real number representing its desirability.'),n("br"),n("strong",null,"Example"),e(": A taxi agent choosing the route that is not just correct, but is also the quickest and most fuel-efficient.")],-1))]),_:1,__:[88]}),a(r,{time:"Level 5",type:"danger",icon:"carbon:machine-learning",card:void 0},{icon:s(()=>[a(o,{provider:"iconify",name:"carbon:machine-learning"})]),title:s(()=>t[89]||(t[89]=[e("Learning Agent",-1)])),default:s(()=>[t[90]||(t[90]=n("p",null,[n("strong",null,"Action basis"),e(": All of the above, plus the ability to improve. A learning agent can operate in unknown environments and become more competent than its initial knowledge allows. It has four main components:")],-1)),t[91]||(t[91]=n("p",null,[n("strong",null,"Performance Element"),e(": The agent itself (e.g., a utility-based agent)."),n("br"),n("strong",null,"Critic"),e(": Provides feedback on how well the agent is doing."),n("br"),n("strong",null,"Learning Element"),e(": Uses feedback to modify the performance element."),n("br"),n("strong",null,"Problem Generator"),e(": Suggests exploratory actions to gain new, informative experiences.")],-1))]),_:1,__:[90,91]})]),_:1})])}const E=w(x,[["render",T]]),V=JSON.parse('{"path":"/fit3080/pb1j0vwr/","title":"Intelligent Agents and Rationality","lang":"en-US","frontmatter":{"title":"Intelligent Agents and Rationality","createTime":"2025/08/03 15:10:27","permalink":"/fit3080/pb1j0vwr/"},"readingTime":{"minutes":19.14,"words":1531},"git":{"createdTime":1754661056000,"updatedTime":1754661056000,"changelog":[{"hash":"22b767adcf06b875fd35ceb163512c550a5c21fc","time":1754661056000,"email":"yunfeng.long@yahoo.com","author":"WARREN Y.F. LONG","message":"feat: drop section for agents"}]},"filePathRelative":"notes/fit3080/agents/1.agent_basic.md","headers":[]}');export{E as comp,V as data};
