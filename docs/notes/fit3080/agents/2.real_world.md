---
title: Real World Implementation
createTime: 2025/08/03 17:22:03
permalink: /fit3080/1tgvq8kr/
---

### Implementation and Instructions

This guide provides the complete Python code for a simulation of the **Vacuum-Cleaner World**, a classic AI problem from FIT3080 materials from Monash University.

::: info About This Problem
The Vacuum-Cleaner World is a simple yet powerful example that demonstrates fundamental AI concepts including **agent design**, **environment interaction**, and **problem-solving approaches**.
:::

::: steps

1.  **Save the Code** üíæ
    Copy the code below and save it in a file named `vacuum_agent.py`.

2.  **Run from Terminal** üíª
    Open your terminal or command prompt, navigate to the directory where you saved the file, and run the script using the following command:

    ```sh
    python vacuum_agent.py
    ```

3.  **Observe the Output** üëÅÔ∏è
    The script will run a 10-step simulation and print the agent's percepts, its chosen action, and the resulting state of the world at each step. This allows you to see the agent's decision-making process in action.
    :::

-----

### The Code

```python title="vacuum_agent.py"
import random

def simple_reflex_agent(percept):
    """
    This function implements the simple reflex agent.
    It takes a percept [location, status] and returns an action.
    """
    location, status = percept
    # The agent's logic is based directly on the rules from the lecture.
    if status == 'Dirty':
        return 'Vacuum'
    elif location == 'A':
        return 'Right'
    elif location == 'B':
        return 'Left'

class VacuumCleanerWorld:
    """
    Represents the environment for the vacuum cleaner agent.
    This class manages the state of the world and the agent's position.
    """
    def __init__(self):
        # The world can have two locations, A and B.
        # The status of each can be 'Clean' or 'Dirty'.
        self.world_state = {'A': random.choice(['Clean', 'Dirty']), 'B': random.choice(['Clean', 'Dirty'])}
        self.agent_location = random.choice(['A', 'B'])
        print(f"Initial State: World={self.world_state}, Agent at '{self.agent_location}'")

    def get_percept(self):
        """
        Returns the agent's current percept: [location, status].
        """
        return [self.agent_location, self.world_state[self.agent_location]]

    def perform_action(self, action):
        """
        Updates the environment based on the agent's action.
        """
        if action == 'Vacuum':
            # If the agent vacuums, the current location becomes clean.
            self.world_state[self.agent_location] = 'Clean'
        elif action == 'Right' and self.agent_location == 'A':
            # If the agent moves right from A, it goes to B.
            self.agent_location = 'B'
        elif action == 'Left' and self.agent_location == 'B':
            # If the agent moves left from B, it goes to A.
            self.agent_location = 'A'
        # If the agent tries to move off the edge (e.g., 'Left' from 'A'), nothing happens.

    def is_done(self):
        """
        The task is done if all locations are clean.
        """
        return all(value == 'Clean' for value in self.world_state.values())

# --- Main Simulation Loop ---
if __name__ == "__main__":
    # 1. Create the environment
    env = VacuumCleanerWorld()
    max_steps = 10

    print("\n--- Simulation Starting ---")
    for step in range(max_steps):
        if env.is_done():
            print(f"\nStep {step+1}: GOAL REACHED! The world is clean.")
            break

        # 2. Get the agent's percept from the environment
        current_percept = env.get_percept()

        # 3. The agent chooses an action based on the percept
        action = simple_reflex_agent(current_percept)

        print(f"\nStep {step+1}:")
        print(f"  - Percept: {current_percept}")
        print(f"  - Action: {action}")

        # 4. The agent performs the action, changing the environment
        env.perform_action(action)
        print(f"  - New State: World={env.world_state}, Agent at '{env.agent_location}'")

    if not env.is_done():
        print(f"\n--- Simulation Ended after {max_steps} steps ---")

```

-----

### How the Code Works

The implementation is broken into two main parts that mirror the theoretical concepts of an agent and its environment.

:::: card-grid
::: card title="The Environment: `VacuumCleanerWorld`" icon="mdi:earth"
This class represents the physical world the agent operates in. Its responsibilities are:

- **Maintaining State**: It keeps track of which rooms are dirty or clean and where the agent is currently located.
- **Providing Percepts**: The `get_percept()` method gives the agent its sensory information‚Äîits current location and whether that location is dirty.
- **Updating State**: The `perform_action()` method takes an action from the agent and modifies the world accordingly. For example, if the agent chooses `'Vacuum'`, the class changes the status of the current room to `'Clean'`.
:::

::: card title="The Agent: `simple_reflex_agent()`" icon="mdi:robot"
This function contains the agent's intelligence or "brain." Its responsibilities are:

- **Decision Making**: It implements the agent's logic. It takes a percept as its only input.
- **Stateless Reaction**: As a ==simple reflex agent==, it has no memory of past events. Its decision is based *only* on the current percept, following a set of `if-else` rules to choose an action.
:::
::::

## Revisiting the Vacuum World with Search

::: info Prerequisites
Please read [Solving Problems by Searching](/fit3080/o9dc7deh/) before attempting the following contents.
:::

Instead of a simple reflex agent, we now model a more intelligent agent that formulates the problem and uses a search algorithm to find an optimal solution. This implementation uses the **Graph Search** algorithm with a **Breadth-First Search (BFS)** strategy to find the shortest possible sequence of actions to clean the world.

::: tip Upgrade Benefits
- **Planning**: Agent thinks before acting
- **Optimality**: Finds shortest action sequence  
- **Completeness**: Guaranteed to find solution if one exists
- **No Redundancy**: Avoids revisiting the same states
:::

::: steps

1.  **Problem Formulation** üß©
    First, we define the world as a formal search problem. This includes defining all possible **states**, the available **actions**, a **transition model** that predicts the outcome of an action, and a **goal test** to check if the world is clean.

2.  **Search** üîç
    The agent uses the **Breadth-First Search** algorithm to find a solution. BFS explores the state space level by level, guaranteeing that it finds the shortest sequence of actions because each action has a uniform cost of 1. It uses a **frontier** of nodes to explore and an **explored set** to avoid revisiting states and getting caught in loops.

3.  **Solution** üèÅ
    The output of the search is not just a single action, but a complete **solution**‚Äîa sequence of actions (e.g., `['Vacuum', 'Right', 'Vacuum']`) that the agent can execute to achieve the goal.
    :::

-----

### The Code Implementation

This single Python file contains the full implementation for the problem definition and the search algorithm.

::: warning State Representation
Notice how we use **tuples** to represent states instead of dictionaries. This is because states need to be **hashable** to work with Python sets for the explored collection.
:::

```python title="search_vacuum_agent.py"
import collections

class Node:
    """A node in a search tree. Contains a pointer to the parent (the node
    that this is a successor of) and to the state for this node. Also
    stores the action that got us to this state and the total path cost.
    """
    def __init__(self, state, parent=None, action=None, path_cost=0):
        self.state = state
        self.parent = parent
        self.action = action
        self.path_cost = path_cost

    def __repr__(self):
        return f"<Node {self.state}>"

class VacuumProblem:
    """The problem of cleaning up the vacuum world.
    This class formalizes the problem by defining the states, actions,
    transition model, and goal test."""

    def __init__(self, initial_state):
        self.initial_state = initial_state

    def get_actions(self, state):
        """Returns the actions that can be executed in the given state."""
        return ['Left', 'Right', 'Vacuum']

    def get_result(self, state, action):
        """The transition model. Returns the state that results from doing
        the given action in the given state. """
        # A state is a tuple: (agent_location, room_statuses)
        # e.g., ('A', {'A': 'Dirty', 'B': 'Clean'})
        # We use frozenset for the dict items to make the state hashable.
        location, statuses_tuple = state
        statuses = dict(statuses_tuple)

        if action == 'Vacuum':
            # The room at the agent's location becomes clean
            statuses[location] = 'Clean'
        elif action == 'Left' and location == 'B':
            location = 'A'
        elif action == 'Right' and location == 'A':
            location = 'B'
        
        # Return the new state
        return (location, tuple(sorted(statuses.items())))

    def is_goal(self, state):
        """Returns True if the state is a goal state.
        The goal is for all rooms to be clean. """
        _location, statuses_tuple = state
        statuses = dict(statuses_tuple)
        return all(value == 'Clean' for value in statuses.values())

def breadth_first_search(problem):
    """
    Implements Breadth-First Search (BFS) using the Graph Search algorithm. 
    """
    # A node is created for the initial state. 
    node = Node(problem.initial_state)

    # The frontier is a FIFO queue. 
    frontier = collections.deque([node])
    
    # The explored set stores states that have been visited. 
    explored = set()

    while frontier:
        # Choose the shallowest node in the frontier. 
        node = frontier.popleft()

        # Check if this node's state is the goal. 
        if problem.is_goal(node.state):
            # If it is, reconstruct and return the solution path.
            solution = []
            while node.parent is not None:
                solution.append(node.action)
                node = node.parent
            return solution[::-1] # Reverse to get the correct order

        # Add the state to the explored set. 
        explored.add(node.state)

        # Expand the node by considering all possible actions. 
        for action in problem.get_actions(node.state):
            child_state = problem.get_result(node.state, action)
            child_node = Node(child_state, node, action, node.path_cost + 1)

            # Add the child to the frontier only if it hasn't been explored
            # and isn't already in the frontier.
            if child_state not in explored and child_node not in frontier:
                frontier.append(child_node)

    return None # Return None if no solution is found

# --- Main Simulation ---
if __name__ == "__main__":
    # Define the initial state of the world. 
    # State format: (agent_location, (('A', status), ('B', status)))
    # The inner tuple of tuples is used to make the dictionary part of the state hashable.
    initial = ('A', (('A', 'Dirty'), ('B', 'Dirty')))
    
    # Create the problem instance.
    problem = VacuumProblem(initial)
    
    print(f"Initial State: Agent in '{initial[0]}', Rooms: {dict(initial[1])}")
    print("Searching for a solution using Breadth-First Search...")
    
    # Find the solution.
    solution_path = breadth_first_search(problem)
    
    if solution_path:
        print(f"\nSolution Found! ‚úÖ")
        print(f"Shortest action sequence: {solution_path}")
    else:
        print("\nNo solution found. ‚ùå")

```

-----

### Running the Simulation

When you run this script, it will immediately calculate the best plan and print it.

::: code-group

```sh [Command]
python search_vacuum_agent.py
```

```output [Expected Output]
Initial State: Agent in 'A', Rooms: {'A': 'Dirty', 'B': 'Dirty'}
Searching for a solution using Breadth-First Search...

Solution Found! ‚úÖ
Shortest action sequence: ['Vacuum', 'Right', 'Vacuum']
```

:::

**Sample Output:**

```output
Initial State: Agent in 'A', Rooms: {'A': 'Dirty', 'B': 'Dirty'}
Searching for a solution using Breadth-First Search...

Solution Found! ‚úÖ
Shortest action sequence: ['Vacuum', 'Right', 'Vacuum']
```

::: success Key Achievement
The agent has successfully formulated the problem and found the shortest possible sequence of actions to get the job done **before taking a single step**. This demonstrates the power of planning over reactive behavior.
:::

### Algorithm Comparison

::: tabs

@tab Simple Reflex Agent

**Approach**: React to current percept only

- ‚úÖ Simple implementation
- ‚úÖ Low memory usage  
- ‚ùå No planning ahead
- ‚ùå May take suboptimal paths
- ‚ùå Can get stuck in loops

**Performance**: Variable, depends on environment

@tab Search-Based Agent

**Approach**: Plan entire solution before acting

- ‚úÖ Finds optimal solution
- ‚úÖ Avoids redundant actions
- ‚úÖ Guaranteed completion
- ‚ùå Higher computational cost
- ‚ùå Requires complete world model

**Performance**: Always optimal (3 steps for this problem)

:::

### State Space Visualization

::: details Click to see the complete state space

For the vacuum world with 2 rooms, there are **8 possible states**:

```
State 1: Agent at A, {A: Clean, B: Clean}   [GOAL]
State 2: Agent at A, {A: Clean, B: Dirty}  
State 3: Agent at A, {A: Dirty, B: Clean}  
State 4: Agent at A, {A: Dirty, B: Dirty}   [INITIAL]
State 5: Agent at B, {A: Clean, B: Clean}   [GOAL]
State 6: Agent at B, {A: Clean, B: Dirty}  
State 7: Agent at B, {A: Dirty, B: Clean}  
State 8: Agent at B, {A: Dirty, B: Dirty}  
```

The BFS algorithm explores these systematically to find the shortest path from any initial state to a goal state.

:::