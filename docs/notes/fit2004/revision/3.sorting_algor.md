---
title: Sorting Algorithms
createTime: 2025/06/06 11:21:13
permalink: /fit2004/myoip0uf/
---

### Bubble Sort

:::note Algorithm Analysis
**Correctness**
    - **Loop Invariant:** At the start of each pass `i` through the outer loop, the largest `i-1` elements are in their correct final positions at the end of the array.
    - **Termination:** The outer loop runs `n-1` times, and the inner loop's range decreases with each pass, guaranteeing the algorithm will terminate.
- **Complexity**
    - **Time:** The algorithm consists of two nested loops. In the average and worst cases, it performs O(n2) comparisons and swaps. In the best case (an already sorted array), an optimized version can detect that no swaps were made in the first pass and terminate in O(n) time.
    - **Space:** It requires only a few extra variables for swapping, resulting in O(1) auxiliary space.
:::

### Insertion Sort

:::note Algorithm Analysis
**Correctness**

- Loop invariant
    - my_list[0…i-1] sorted
    - my_list[0…i-1] < my_list[N]
- Termination
    - i increase by 1 and j (which is i level) is decrease by 1 in the inner loop until i reach to the length of the list

Complexity

- Time
    - Best: O(kn)
        - Each loop only look and compare with left item once
    - Worst: O(kn^2)
        - Each loop keep look left, compare and swap till beginning of list
- Space
    - O(n) for the input list
    - Auxiliary: O(1) → in-place

Stability: Yes → Don’t swap if value is the same
:::

```python
def insertion_sort(my_list):
    for i in range(1, len(my_list)):
        key = my_list[i]
        j = i - 1
        while j >= 0 and key < my_list[j]:
            my_list[j + 1] = my_list[j]
            j -= 1
        my_list[j + 1] = key
    return my_list
```
### Selection Sort

:::note Algorithm Analysis
**Correctness**

- Loop invariant
    - array[0…i-1] is sorted
    - For any x in array[1..i−1] and y in array[i..n], x ≤ y
- Termination
    - i and j always increment and both reach the end of the list

Complexity

- Time
    - Best: $O(kn^2)$, because no matter what we have to find the minimum
    and cant terminate earlier!
    - Worst: $O(kn^2)$
    - where O(k) is the comparison cost
- Space
    - $O(n)$ for the input list
    - Auxiliary: O(1) → in-place

Stable: No

- [4a, 2, 3, 4b, 1]
- Minimum is 1, so we swap
- [1, 2, 3, 4b, 4a]
- Now we see that 4a is behind 4b!
:::

```python
def selection_sort(my_list):
    for i in range(len(my_list)):
        min_index = i
        for j in range(i + 1, len(my_list)):
            if my_list[j] < my_list[min_index]:
                min_index = j
        my_list[i], my_list[min_index] = my_list[min_index], my_list[i]
    return my_list
```

### Merge Sort

:::note Algorithm Analysis
**Correctness**

- Loop invariant
    - At the start of each iteration of the loop that merges two sorted subarrays, the output array contains the smallest elements from the unmerged parts of the subarrays, in sorted order.
- Termination
    - The recursive calls are made on progressively smaller subarrays until the base case (an array of size 1) is reached. The merge step iterates through all elements. The algorithm is guaranteed to terminate.

Complexity

- Time
    - The recurrence relation for Merge Sort is $T(n)=2T(n/2)+O(n)$. This is because it divides the problem into two subproblems of half the size and does $O(n)$ work to merge the results. Solving this via the Master Theorem gives a consistent
- Space
    - The loop terminates when i and j cross. The function then returns j as the partition point.
:::

### Quick Sort

:::note Algorithm Analysis
**Correctness**

- Loop invariant
    - At each iteration of the partitioning process, elements to the left of the pivot are less than or equal to the pivot, and elements to the right are greater than or equal to it.
    - **Invariant for Partitioning**
        - **Left <= pivot**
        - **Right > pivot**
- Termination
    - Each recursive call is on a smaller subarray. The algorithm terminates when the subarrays are empty or contain one element.

Complexity

- Time
    - The best and average cases occur when the pivot selection consistently divides the array into two roughly equal halves, leading to the recurrence $T(n)=2T(n/2)+O(n)$ and a complexity of $O(nlogn)$. The worst case occurs when the pivot is always the smallest or largest element, creating highly unbalanced partitions. This leads to the recurrence $T(n)=T(n−1)+O(n)$, which results in $O(n^2)$ complexity.
- Space
    - The space is used by the recursion stack. In the best and average cases, the recursion depth is $O(logn)$. In the worst case, it can be $O(n)$.
:::

#### Hoare's Scheme for QuickSort

The process can be outlined as:

1. Choose a Pivot: Typically, the first element of the partition is chosen as the pivot. 
2. Partitioning:
    1. Initialize a left pointer i just before the start of the partition and a right pointer j just after the end.
    2. Move i to the right until you find an element arr[i] that is greater or equal to the pivot.
    3. Move j to the left until you find an element arr[j] that is less than or equal to the pivot.
    4. If i is still less than j, swap arr[i] and arr[j]
3. Termination: The loop terminates when i and j cross. The function then returns j as the partition point.
4. Recursive Calls: Recursive Calls: Recursively apply QuickSort to the two sub-arrays: from the original start to j, and from j + 1 to the original end.

```python
def quick_sort_hoare(arr):
    """
    Sorts an array in place using the QuickSort algorithm with Hoare's partition scheme.
    This implementation includes a special check to prevent infinite loops on edge cases.
    """
    def _quick_sort(items, low, high):
        if low < high:
            # p is the partitioning index. Note that arr[p] is not necessarily
            # in its final sorted position, unlike in Lomuto's scheme.
            p = _partition(items, low, high)

            # Recursively sort the two partitions.
            # It's crucial to use p as the upper bound for the first recursive
            # call, not p - 1, to handle the case where the pivot is the smallest element.
            _quick_sort(items, low, p)
            _quick_sort(items, p + 1, high)

    def _partition(items, low, high):
        # Choose the middle element as the pivot to improve performance
        # on partially sorted arrays.
        pivot = items[(low + high) // 2]
        i = low - 1
        j = high + 1

        while True:
            # Find an element on the left side that should be on the right side
            i += 1
            while items[i] < pivot:
                i += 1

            # Find an element on the right side that should be on the left side
            j -= 1
            while items[j] > pivot:
                j -= 1

            # If the pointers have crossed, the partition is done
            if i >= j:
                return j

            # Swap the elements to move them to the correct side
            items[i], items[j] = items[j], items[i]

    # Initial call to the recursive sort function
    _quick_sort(arr, 0, len(arr) - 1)
```

#### Lomuto's Scheme for QuickSort

The process can be outlined as:

1. Choose a Pivot: Typically, the last element in the partition is chosen as the pivot.
2. Partitioning:
    1. Initialize a left pointer i just before the start of the partition and a right pointer j just after the end.
    2. Iterate through the partition from the start to the element just before the pivot with an index j.
    3. If the element at arr[j] is less then or equal to the pivot, increment i and swap arr[i] with arr[j]
3. Final Pivot Placement: After the loop, swap the pivot element (arr[high]) with the element at arr[i+1].
4. Recursive Calls: Recursively apply QuickSort to the sub-arrays to the left and right of the pivot's new position.

```python
def quick_sort_lomuto(arr):
    """
    Sorts an array in place using the QuickSort algorithm with Lomuto's partition scheme.
    """
    def _quick_sort(items, low, high):
        if low < high:
            # pi is the partitioning index, arr[pi] is now at the right place
            pi = _partition(items, low, high)

            # Separately sort elements before partition and after partition
            _quick_sort(items, low, pi - 1)
            _quick_sort(items, pi + 1, high)

    def _partition(items, low, high):
        # Choose the last element as the pivot
        pivot = items[high]
        
        # i is the index of the smaller element
        i = low - 1

        for j in range(low, high):
            # If the current element is smaller than or equal to the pivot
            if items[j] <= pivot:
                i += 1
                items[i], items[j] = items[j], items[i]
        
        # Place the pivot at its correct position
        items[i + 1], items[high] = items[high], items[i + 1]
        return i + 1

    # Initial call to the recursive sort function
    _quick_sort(arr, 0, len(arr) - 1)
```

#### Dutch National Flag Algorithm

Correctness

- Loop invariant
    - array[1..lo−1] contains the red items
    - array[lo..mid−1] contains the white items
    - array[mid..hi] contains the currently unknown items
    - array[hi + 1..n] contains the blue items

```python
def quick_sort_with_dnf(arr):
    """
    Sorts an array in place using the QuickSort algorithm, combined with
    the Dutch National Flag (3-way) partitioning scheme.

    This is the main function that the user calls.
    """
    _quick_sort_recursive(arr, 0, len(arr) - 1)

def _partition_3_way(arr, low, high):
    """
    Partitions the array segment arr[low...high] using the Dutch National Flag logic.
    This function is called by the QuickSort driver.
    
    It rearranges the array into three parts:
    - arr[low...lt-1] < pivot
    - arr[lt...gt]   == pivot
    - arr[gt+1...high] > pivot

    Returns:
        A tuple (lt, gt) representing the boundaries of the 'equal to' partition.
    """
    # For robustness, we choose a random pivot and move it to the start.
    # This avoids worst-case O(n^2) behavior on already-sorted arrays.
    pivot_index = random.randint(low, high)
    arr[low], arr[pivot_index] = arr[pivot_index], arr[low]
    pivot = arr[low]
    
    lt = low      # Pointer for the end of the 'less than' section
    i = low       # The main scanning pointer
    gt = high     # Pointer for the start of the 'greater than' section

    while i <= gt:
        if arr[i] < pivot:
            # Element is smaller than pivot, move to 'less than' section
            arr[lt], arr[i] = arr[i], arr[lt]
            lt += 1
            i += 1
        elif arr[i] > pivot:
            # Element is greater than pivot, move to 'greater than' section
            arr[i], arr[gt] = arr[gt], arr[i]
            gt -= 1
            # 'i' does not advance since the new arr[i] is unknown
        else: # arr[i] == pivot
            # Element is equal to pivot, leave it in the middle section
            i += 1
            
    return lt, gt

def _quick_sort_recursive(arr, low, high):
    """
    The recursive QuickSort driver that uses the 3-way partition.
    """
    # Base case: if the partition has 0 or 1 elements, it's already sorted.
    if low >= high:
        return

    # This is where the DNF partition method is CALLED
    lt, gt = _partition_3_way(arr, low, high)

    # Recursively call QuickSort on the two outer partitions
    # 1. The "less than pivot" part
    _quick_sort_recursive(arr, low, lt - 1)
    
    # 2. The "greater than pivot" part
    _quick_sort_recursive(arr, gt + 1, high)
    
    # Note: The middle part (from lt to gt) is never touched again,
    # as it's already sorted.
```

### Counting Sort

:::note Algorithm Analysis
**Correctness**

- Loop invariant
    - After the initial counting loop, the auxiliary array C stores the count of each element. After the second loop (modifying C), C[i] stores the number of elements less than or equal to i. The final loop places elements into their correct sorted positions based on the counts in C.
- Termination
    - The algorithm consists of three non-nested loops that iterate through the input array or the range of key values (k). It does not involve recursion and is guaranteed to terminate.

Complexity

- Time
    - The algorithm iterates through the input array once to count, once through the key range to sum counts, and once more through the input array to place elements. This gives a total time of O(n+k), where n is the number of elements and k is the range of the element values.
- Space
    - It requires an auxiliary array of size k to store counts and another of size n for the output, leading to O(n+k) auxiliary space.
:::
    
```python
def counting_sort(my_list, m):
        max_val = max(my_list)
        count = [0] * (max_val + 1)
        for num in my_list:
            count[num] += 1
        sorted_list = []
        for i in range(len(count)):
            sorted_list.extend([i] * count[i])
        return sorted_list
    ```
    

### Radix Sort

:::note Algorithm Analysis
**Correctness**

- Loop invariant
    - At the end of each iteration i, the elements are correctly sorted according to their last i digits (least significant digits). This relies on the underlying sort (like Counting Sort) being stable.
- Termination
    - The algorithm iterates once for each digit of the longest number (d times). Since d is a fixed number, the algorithm terminates.

Complexity

- Time
- Space
:::

```python
def radix_sort(my_list):
    def counting_sort_for_radix(arr, exp):
        n = len(arr)
        output = [0] * n
        count = [0] * 10

        for i in range(n):
            index = arr[i] // exp
            count[index % 10] += 1

        for i in range(1, 10):
            count[i] += count[i - 1]

        for i in range(n - 1, -1, -1):
            index = arr[i] // exp
            output[count[index % 10] - 1] = arr[i]
            count[index % 10] -= 1

        for i in range(n):
            arr[i] = output[i]

    max_val = max(my_list)
    exp = 1
    while max_val // exp > 0:
        counting_sort_for_radix(my_list, exp)
        exp *= 10
    return my_list
```

### Summary of Sorting Algorithms

:::tip Complexity Comparison

| Algorithm | Best | Average | Worst | Stable | Space | In-place |
| --- | --- | --- | --- | --- | --- | --- |
| **Bubble** | $O(N)$ (optimized) | $O(N^2)$ | $O(N^2)$ | Yes | $O(1)$ | Yes |
| **Selection** | $O(N^2)$ | $O(N^2)$ | $O(N^2)$ | No | $O(1)$ | Yes |
| **Insertion** | $O(N)$ | $O(N^2)$ | $O(N^2)$ | Yes | $O(1)$ | Yes |
| **Merge** | $O(N \log N)$ | $O(N \log N)$ | $O(N \log N)$ | Yes | $O(N)$ | No |
| **Heap** | $O(N)$ | $O(N \log N)$ | $O(N \log N)$ | No | $O(1)$ | Yes |
| **Quick** | $O(N \log N)$ | $O(N \log N)$ | $O(N^2)$ | No | $O(\log N)$ | Yes |
| **Counting** | $O(N + k)$ | $O(N + k)$ | $O(N + k)$ | Yes | $O(N + k)$ | No |
| **Radix** | $O(d(N + b))$ | $O(d(N + b))$ | $O(d(N + b))$ | Yes | $O(N + b)$ | No |

**Legend:**
- $N$: number of elements
- $k$: range of input values  
- $d$: number of digits
- $b$: base of number system

:::



### QuickSelect

:::important Algorithm Steps

1. **Choose a Pivot:** Select an element from the list to be the "pivot"
2. **Partition:** All elements smaller than the pivot are moved to its left, larger elements to its right
3. **Compare and Recurse:** After partitioning, check the pivot position:
   - If `pivot_index == k-1`: Found the k-th smallest element ✅
   - If `k-1 < pivot_index`: Recurse on the ==left sub-array==
   - If `k-1 > pivot_index`: Recurse on the ==right sub-array==
:::

:::tip Time Complexity
- **Average Case:** $O(n)$ - each partitioning step reduces the sub-array size by a constant factor
- **Worst Case:** $O(n^2)$ - when pivot is always the smallest or largest element (highly unbalanced partitions)
:::

```python
import random

def quickselect(arr, k):
    """
    Finds the k-th smallest element in an array using the Quickselect algorithm.

    Args:
        arr: The list of numbers.
        k: The desired rank (e.g., k=1 for the smallest, k=len(arr) for the largest).

    Returns:
        The k-th smallest element in the array.
    """
    if not (1 <= k <= len(arr)):
        raise ValueError("k is out of the valid range of array indices.")

    return _quickselect(arr, 0, len(arr) - 1, k - 1)

def _quickselect(arr, left, right, k):
    """
    Helper function to perform the recursive Quickselect.
    """
    if left == right:
        return arr[left]

    # Choose a random pivot index to avoid the worst-case scenario
    pivot_index = random.randint(left, right)
    pivot_index = _partition(arr, left, right, pivot_index)

    if k == pivot_index:
        return arr[k]
    elif k < pivot_index:
        return _quickselect(arr, left, pivot_index - 1, k)
    else:
        return _quickselect(arr, pivot_index + 1, right, k)

def _partition(arr, left, right, pivot_index):
    """
    Partitions the array around the pivot.
    """
    pivot_value = arr[pivot_index]
    # Move pivot to the end
    arr[pivot_index], arr[right] = arr[right], arr[pivot_index]
    store_index = left

    for i in range(left, right):
        if arr[i] < pivot_value:
            arr[store_index], arr[i] = arr[i], arr[store_index]
            store_index += 1

    # Move pivot to its final sorted position
    arr[right], arr[store_index] = arr[store_index], arr[right]
    return store_index
```

### Median of Medians Algorithm

The median of medians algorithm is a clever method for selecting a pivot that is *guaranteed* to be "good enough." It doesn't necessarily find the absolute best pivot, but it ensures that the chosen pivot is not the worst-case scenario. It guarantees that the pivot is greater than a certain fraction of the elements and smaller than another certain fraction, thus preventing the highly unbalanced partitions that lead to the $O(n^2)$ worst-case time.

The steps can be break down as follow: 

1. **Divide into Sublists:** Divide the original list of `n` elements into groups of 5 elements each. The last group may have fewer than 5 elements if `n` is not a multiple of 5.
2. **Find the Median of Each Sublist:** For each of these small sublists, find its median. Since the sublists have at most 5 elements, finding their medians is a computationally inexpensive operation. You can simply sort each small sublist and pick the middle element.
3. **Recursively Find the Median of Medians:** Now, you have a new list composed of the medians from each of the sublists. Recursively call the median of medians algorithm on this new list to find *its* median. This final value is your chosen pivot for the original list.

Why Does This Guarantee a Good Pivot?

Let’s call our pivot p:

- By definition, p is the median of the list of medians. This means that half of the medians are smaller than or equal to p, and half are larger than or equal to p.
- Now, consider one of the sublists whose median is smaller than p. Since it’s the median of a 5-element group, at least 3 of these elements (including the median itself) are smaller than or equal to that median.
- Combining these facts, we know that for half of the sublists, at least 3 of their elements are smaller than or equal to p.

This guarantees that our chosen pivot p is greater than at least roughly 30% of the elements and smaller than at least roughly 30% of the elements. This is because at least 3 elements from each of the n/10 sublists are smaller than p (3 * n/10 = 0.3n). This ensures that each recursive step in QuickSelect will discard at least about 30% of the remaining elements, which is sufficient to maintain a linear-time $O(n)$ complexity even in the worst case.

```python
def median_of_medians(arr):
    """
    Finds a good pivot element using the median of medians algorithm.
    """
    if len(arr) <= 5:
        return sorted(arr)[len(arr) // 2]

    # 1. Divide the array into chunks of 5
    chunks = [arr[i:i + 5] for i in range(0, len(arr), 5)]

    # 2. Find the median of each chunk
    medians = [sorted(chunk)[len(chunk) // 2] for chunk in chunks]

    # 3. Recursively find the median of the medians
    return median_of_medians(medians)
```

#### QuickSelect with Median of Medians

```python
def quickselect_with_median_of_medians(arr, k):
    """
    Finds the k-th smallest element using Quickselect with the median of medians pivot.
    k is 1-based for user convenience.
    """
    if not (1 <= k <= len(arr)):
        raise ValueError("k is out of the valid range of array indices.")

    k_zero_based = k - 1

    if len(arr) <= 5:
        return sorted(arr)[k_zero_based]

    pivot = median_of_medians(arr)

    # Partition the array around the pivot
    low = [x for x in arr if x < pivot]
    high = [x for x in arr if x > pivot]
    pivots = [x for x in arr if x == pivot]

    if k_zero_based < len(low):
        return quickselect_with_median_of_medians(low, k)
    elif k_zero_based < len(low) + len(pivots):
        return pivots[0]
    else:
        new_k = k - (len(low) + len(pivots))
        return quickselect_with_median_of_medians(high, new_k)
```